{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c283c4c7",
   "metadata": {},
   "source": [
    "## Step 1: Clone Repository & Install Dependencies\n",
    "\n",
    "This clones the BLIP+PVT repository and installs all required packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c66b6c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the repository\n",
    "!git clone https://github.com/ribhu0105-alt/blip-using-pvt-cbam.git\n",
    "%cd blip-using-pvt-cbam\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb1a0987",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies (quiet mode to reduce output)\n",
    "!pip install -q -r requirements.txt\n",
    "print(\"✓ Dependencies installed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b36d02",
   "metadata": {},
   "source": [
    "## Step 2: Verify Installation\n",
    "\n",
    "Test that all imports work and models can be loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df6c872",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python test_import.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "893d5683",
   "metadata": {},
   "source": [
    "## Step 3: Mount Google Drive or Upload Dataset\n",
    "\n",
    "**Choose ONE of the following:**\n",
    "\n",
    "### Option A: Mount Google Drive (recommended for large datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "925d7fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "print(\"✓ Google Drive mounted\")\n",
    "print(\"\\nYour files are in: /content/drive/MyDrive/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac00591e",
   "metadata": {},
   "source": [
    "### Option B: Upload from local machine (for smaller datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a39a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "import zipfile\n",
    "import os\n",
    "\n",
    "print(\"Select your dataset ZIP file (images + captions.txt)\")\n",
    "uploaded = files.upload()\n",
    "\n",
    "# Extract ZIP\n",
    "for filename in uploaded.keys():\n",
    "    if filename.endswith('.zip'):\n",
    "        with zipfile.ZipFile(filename, 'r') as zip_ref:\n",
    "            zip_ref.extractall('/content/dataset')\n",
    "        print(f\"✓ Extracted {filename}\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a56d7ad",
   "metadata": {},
   "source": [
    "## Step 4: Prepare Dataset\n",
    "\n",
    "Your dataset should have:\n",
    "- **Images folder**: `images/` with .jpg, .png files\n",
    "- **Captions file**: `captions.txt` with format:\n",
    "  ```\n",
    "  image_0001.jpg\\ta dog running in the park\n",
    "  image_0002.jpg\\ta cat sleeping on a bed\n",
    "  image_0003.jpg\\tpeople sitting on a bench\n",
    "  ```\n",
    "\n",
    "### Configure paths based on your setup:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ab323b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# CONFIGURE THESE PATHS FOR YOUR DATASET\n",
    "# ============================================\n",
    "\n",
    "# Option 1: If using Google Drive (uncomment and adjust)\n",
    "# IMAGE_ROOT = \"/content/drive/MyDrive/your_dataset/images\"\n",
    "# CAPTION_FILE = \"/content/drive/MyDrive/your_dataset/captions.txt\"\n",
    "\n",
    "# Option 2: If using uploaded ZIP (uncomment)\n",
    "# IMAGE_ROOT = \"/content/dataset/images\"\n",
    "# CAPTION_FILE = \"/content/dataset/captions.txt\"\n",
    "\n",
    "# Option 3: Sample/test path (for verification)\n",
    "IMAGE_ROOT = \"/content/blip-using-pvt-cbam/data/sample_images\"  # Change this\n",
    "CAPTION_FILE = \"/content/blip-using-pvt-cbam/data/captions.txt\"  # Change this\n",
    "\n",
    "OUTPUT_DIR = \"/content/checkpoints\"\n",
    "\n",
    "print(f\"Images folder: {IMAGE_ROOT}\")\n",
    "print(f\"Captions file: {CAPTION_FILE}\")\n",
    "print(f\"Output dir: {OUTPUT_DIR}\")\n",
    "\n",
    "# Verify paths exist\n",
    "import os\n",
    "if os.path.exists(IMAGE_ROOT):\n",
    "    print(f\"✓ Images folder found ({len(os.listdir(IMAGE_ROOT))} files)\")\n",
    "else:\n",
    "    print(f\"⚠ Images folder NOT found. Create it or adjust path.\")\n",
    "\n",
    "if os.path.exists(CAPTION_FILE):\n",
    "    with open(CAPTION_FILE) as f:\n",
    "        lines = f.readlines()\n",
    "    print(f\"✓ Captions file found ({len(lines)} captions)\")\n",
    "else:\n",
    "    print(f\"⚠ Captions file NOT found. Create it or adjust path.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df2ac31",
   "metadata": {},
   "source": [
    "### Create sample dataset (for testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f82cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This creates a small sample dataset for testing\n",
    "# Skip this if you have your own dataset\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "# Create sample folder\n",
    "sample_dir = \"/content/blip-using-pvt-cbam/data/sample_images\"\n",
    "os.makedirs(sample_dir, exist_ok=True)\n",
    "\n",
    "# Create 10 random sample images\n",
    "sample_captions = [\n",
    "    \"a dog running in the park\",\n",
    "    \"a cat sleeping on a bed\",\n",
    "    \"people sitting on a bench\",\n",
    "    \"a sunset over mountains\",\n",
    "    \"a forest path in nature\",\n",
    "    \"a city street at night\",\n",
    "    \"a beach with waves\",\n",
    "    \"a bird flying in sky\",\n",
    "    \"a flower in bloom\",\n",
    "    \"a car parked on street\",\n",
    "]\n",
    "\n",
    "for i, caption in enumerate(sample_captions):\n",
    "    # Create random RGB image\n",
    "    img_array = np.random.randint(0, 256, (384, 384, 3), dtype=np.uint8)\n",
    "    img = Image.fromarray(img_array, mode='RGB')\n",
    "    img.save(f\"{sample_dir}/sample_{i:04d}.jpg\")\n",
    "\n",
    "# Create captions file\n",
    "caption_file = \"/content/blip-using-pvt-cbam/data/captions.txt\"\n",
    "with open(caption_file, 'w') as f:\n",
    "    for i, caption in enumerate(sample_captions):\n",
    "        f.write(f\"sample_{i:04d}.jpg\\t{caption}\\n\")\n",
    "\n",
    "print(f\"✓ Created {len(sample_captions)} sample images in {sample_dir}\")\n",
    "print(f\"✓ Created captions file: {caption_file}\")\n",
    "print(\"\\nSample captions:\")\n",
    "for caption in sample_captions[:3]:\n",
    "    print(f\"  - {caption}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c188f8",
   "metadata": {},
   "source": [
    "## Step 5: Training\n",
    "\n",
    "This trains the BLIP+PVT model on your dataset. Adjust parameters as needed:\n",
    "- `batch_size`: Use 4 for free Colab, 8 for Colab Pro\n",
    "- `epochs`: 5-10 recommended. Start with 2 for testing\n",
    "- `use_amp`: Enables automatic mixed precision (saves memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e75cb9a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# ADJUST THESE FOR YOUR TRAINING\n",
    "# ============================================\n",
    "\n",
    "BATCH_SIZE = 4          # Use 4 for free Colab, 8 for Pro\n",
    "EPOCHS = 2              # Start with 2 for testing, 10 for production\n",
    "LEARNING_RATE = 1e-4    # Standard learning rate\n",
    "IMAGE_SIZE = 384        # BLIP standard image size\n",
    "USE_AMP = True          # Automatic mixed precision (saves GPU memory)\n",
    "NUM_WORKERS = 2         # Data loader workers\n",
    "\n",
    "print(f\"Training Configuration:\")\n",
    "print(f\"  Batch size: {BATCH_SIZE}\")\n",
    "print(f\"  Epochs: {EPOCHS}\")\n",
    "print(f\"  Learning rate: {LEARNING_RATE}\")\n",
    "print(f\"  Image size: {IMAGE_SIZE}x{IMAGE_SIZE}\")\n",
    "print(f\"  Mixed precision: {USE_AMP}\")\n",
    "print(f\"  Output directory: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b80bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training\n",
    "import subprocess\n",
    "import os\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "cmd = [\n",
    "    \"python\", \"train_caption_pvt.py\",\n",
    "    \"--image_root\", IMAGE_ROOT,\n",
    "    \"--caption_file\", CAPTION_FILE,\n",
    "    \"--batch_size\", str(BATCH_SIZE),\n",
    "    \"--epochs\", str(EPOCHS),\n",
    "    \"--lr\", str(LEARNING_RATE),\n",
    "    \"--image_size\", str(IMAGE_SIZE),\n",
    "    \"--output_dir\", OUTPUT_DIR,\n",
    "    \"--num_workers\", str(NUM_WORKERS),\n",
    "]\n",
    "\n",
    "if USE_AMP:\n",
    "    cmd.append(\"--use_amp\")\n",
    "\n",
    "print(\"Starting training...\")\n",
    "print(f\"Command: {' '.join(cmd)}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "result = subprocess.run(cmd, cwd=\"/content/blip-using-pvt-cbam\")\n",
    "print(\"=\"*60)\n",
    "if result.returncode == 0:\n",
    "    print(\"✓ Training completed successfully!\")\n",
    "else:\n",
    "    print(\"✗ Training failed. Check error messages above.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ffdadcf",
   "metadata": {},
   "source": [
    "## Step 6: List Available Checkpoints\n",
    "\n",
    "Find the checkpoint to use for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6288a4b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "checkpoints = sorted(glob.glob(f\"{OUTPUT_DIR}/*.pth\"))\n",
    "\n",
    "if checkpoints:\n",
    "    print(f\"Found {len(checkpoints)} checkpoints:\")\n",
    "    for ckpt in checkpoints[-5:]:  # Show last 5\n",
    "        size_mb = os.path.getsize(ckpt) / (1024*1024)\n",
    "        print(f\"  - {os.path.basename(ckpt)} ({size_mb:.1f} MB)\")\n",
    "    \n",
    "    # Use the latest checkpoint\n",
    "    CHECKPOINT = checkpoints[-1]\n",
    "    print(f\"\\n✓ Using latest: {os.path.basename(CHECKPOINT)}\")\n",
    "else:\n",
    "    print(\"⚠ No checkpoints found. Run training first.\")\n",
    "    CHECKPOINT = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e0967c",
   "metadata": {},
   "source": [
    "## Step 7: Single Image Inference\n",
    "\n",
    "Generate a caption for a single image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d0a1e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download a sample image for testing\n",
    "import urllib.request\n",
    "from PIL import Image\n",
    "import io\n",
    "\n",
    "TEST_IMAGE_URL = \"https://raw.githubusercontent.com/pytorch/hub/master/images/dog.jpg\"\n",
    "TEST_IMAGE_PATH = \"/content/test_image.jpg\"\n",
    "\n",
    "try:\n",
    "    print(f\"Downloading test image from: {TEST_IMAGE_URL}\")\n",
    "    with urllib.request.urlopen(TEST_IMAGE_URL, timeout=10) as response:\n",
    "        image_data = response.read()\n",
    "    \n",
    "    with open(TEST_IMAGE_PATH, 'wb') as f:\n",
    "        f.write(image_data)\n",
    "    \n",
    "    img = Image.open(TEST_IMAGE_PATH)\n",
    "    print(f\"✓ Downloaded image: {img.size}\")\n",
    "except Exception as e:\n",
    "    print(f\"✗ Failed to download: {e}\")\n",
    "    print(\"Using a local sample image instead...\")\n",
    "    TEST_IMAGE_PATH = f\"{IMAGE_ROOT}/sample_0000.jpg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f321721",
   "metadata": {},
   "outputs": [],
   "source": [
    "if CHECKPOINT is None:\n",
    "    print(\"⚠ No checkpoint available. Train first or provide a checkpoint path.\")\n",
    "else:\n",
    "    import subprocess\n",
    "    \n",
    "    cmd = [\n",
    "        \"python\", \"predict_caption.py\",\n",
    "        \"--image\", TEST_IMAGE_PATH,\n",
    "        \"--checkpoint\", CHECKPOINT,\n",
    "        \"--max_length\", \"50\",\n",
    "        \"--num_beams\", \"5\",\n",
    "    ]\n",
    "    \n",
    "    print(f\"Running inference on: {TEST_IMAGE_PATH}\")\n",
    "    print(\"=\"*60)\n",
    "    result = subprocess.run(cmd, cwd=\"/content/blip-using-pvt-cbam\")\n",
    "    print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a15d1c",
   "metadata": {},
   "source": [
    "## Step 8: Batch Inference on Multiple Images\n",
    "\n",
    "Process multiple images and save results to a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cbb896a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure batch inference\n",
    "BATCH_OUTPUT_FILE = \"/content/batch_results.txt\"\n",
    "NUM_TEST_IMAGES = 5  # Number of test images\n",
    "\n",
    "if CHECKPOINT is None:\n",
    "    print(\"⚠ No checkpoint available. Train first.\")\n",
    "else:\n",
    "    import subprocess\n",
    "    \n",
    "    # Get sample images from your dataset\n",
    "    import os\n",
    "    import glob\n",
    "    \n",
    "    test_images = sorted(glob.glob(f\"{IMAGE_ROOT}/*.jpg\"))[:NUM_TEST_IMAGES]\n",
    "    test_images += sorted(glob.glob(f\"{IMAGE_ROOT}/*.png\"))[:max(0, NUM_TEST_IMAGES - len(test_images))]\n",
    "    \n",
    "    print(f\"Running batch inference on {len(test_images)} images...\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    with open(BATCH_OUTPUT_FILE, 'w') as out:\n",
    "        for i, img_path in enumerate(test_images, 1):\n",
    "            print(f\"[{i}/{len(test_images)}] Processing: {os.path.basename(img_path)}\")\n",
    "            \n",
    "            cmd = [\n",
    "                \"python\", \"predict_caption.py\",\n",
    "                \"--image\", img_path,\n",
    "                \"--checkpoint\", CHECKPOINT,\n",
    "                \"--max_length\", \"50\",\n",
    "                \"--num_beams\", \"5\",\n",
    "            ]\n",
    "            \n",
    "            result = subprocess.run(\n",
    "                cmd,\n",
    "                cwd=\"/content/blip-using-pvt-cbam\",\n",
    "                capture_output=True,\n",
    "                text=True\n",
    "            )\n",
    "            \n",
    "            # Parse caption from output\n",
    "            caption_line = [l for l in result.stdout.split('\\n') if 'Generated caption' in l]\n",
    "            if caption_line:\n",
    "                caption = caption_line[0].replace(\"Generated caption:\\n\", \"\").strip()\n",
    "            else:\n",
    "                caption = \"[Failed to generate caption]\"\n",
    "            \n",
    "            out.write(f\"Image: {os.path.basename(img_path)}\\n\")\n",
    "            out.write(f\"Caption: {caption}\\n\\n\")\n",
    "            print(f\"  Caption: {caption}\")\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(f\"✓ Results saved to: {BATCH_OUTPUT_FILE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc24985",
   "metadata": {},
   "source": [
    "## Step 9: View Results\n",
    "\n",
    "Display the batch inference results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a7dc3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if os.path.exists(BATCH_OUTPUT_FILE):\n",
    "    print(\"BATCH INFERENCE RESULTS\")\n",
    "    print(\"=\"*60)\n",
    "    with open(BATCH_OUTPUT_FILE) as f:\n",
    "        print(f.read())\n",
    "else:\n",
    "    print(f\"Results file not found: {BATCH_OUTPUT_FILE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e69dec",
   "metadata": {},
   "source": [
    "## Step 10: Download Results\n",
    "\n",
    "Download all results and checkpoints to your local machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9deab9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "print(\"Preparing files for download...\\n\")\n",
    "\n",
    "# Download batch results\n",
    "if os.path.exists(BATCH_OUTPUT_FILE):\n",
    "    print(f\"Downloading: {os.path.basename(BATCH_OUTPUT_FILE)}\")\n",
    "    files.download(BATCH_OUTPUT_FILE)\n",
    "\n",
    "# Download latest checkpoint\n",
    "if CHECKPOINT and os.path.exists(CHECKPOINT):\n",
    "    print(f\"Downloading: {os.path.basename(CHECKPOINT)}\")\n",
    "    files.download(CHECKPOINT)\n",
    "\n",
    "print(\"\\n✓ Download complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "977b9766",
   "metadata": {},
   "source": [
    "## Summary & Next Steps\n",
    "\n",
    "### What you accomplished:\n",
    "1. ✓ Cloned and set up BLIP+PVT from GitHub\n",
    "2. ✓ Installed all dependencies\n",
    "3. ✓ Trained the model on your dataset\n",
    "4. ✓ Generated captions for test images\n",
    "5. ✓ Downloaded results\n",
    "\n",
    "### Results Quality:\n",
    "- **2 epochs on 10 images**: Basic captions, some noise\n",
    "- **5-10 epochs on 30K images**: Good quality, sensible captions\n",
    "- **15+ epochs on 100K+ images**: Excellent quality\n",
    "\n",
    "### Tips for Better Results:\n",
    "1. **More data**: Train on 30K-100K images instead of 10\n",
    "2. **More epochs**: Use 10-20 epochs for convergence\n",
    "3. **Larger batch size**: Use batch_size=8 if GPU memory allows\n",
    "4. **Fine-tuning**: Use a pre-distilled checkpoint if available\n",
    "5. **Real captions**: Use professional captions (Flickr30K, COCO) instead of synthetic\n",
    "\n",
    "### Troubleshooting:\n",
    "- **Out of memory**: Reduce `batch_size` to 2 or 1\n",
    "- **Slow training**: Reduce `num_workers` to 0 or 1\n",
    "- **Bad captions**: Train for more epochs (5-10 minimum)\n",
    "- **Failed downloads**: Some URLs may be blocked; use local images instead"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
