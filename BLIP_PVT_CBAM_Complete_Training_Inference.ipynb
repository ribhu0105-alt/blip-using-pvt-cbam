{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e13525a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 1: Setup & Installation\n",
    "\n",
    "Clone the repository and install all dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e5e137c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone repository\n",
    "!git clone https://github.com/ribhu0105-alt/blip-using-pvt-cbam.git\n",
    "%cd blip-using-pvt-cbam\n",
    "!pwd\n",
    "print(\"✓ Repository cloned\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afe04a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies (quiet mode)\n",
    "!pip install -q -r requirements.txt\n",
    "print(\"✓ Dependencies installed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a00a681",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify installation\n",
    "!python test_import.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba17d3e9",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 2: Check GPU Availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af7a0b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "if device == 'cuda':\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    print(\"⚠️ No GPU found. Training will be slow. Enable GPU in Runtime > Change runtime type > GPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c90a5ed",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 3: Mount Google Drive (Optional)\n",
    "\n",
    "If your dataset is in Google Drive, mount it here.\n",
    "\n",
    "**Skip this if:** You'll upload files directly or use sample dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "955cd97b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "\n",
    "drive.mount('/content/drive')\n",
    "print(\"✓ Google Drive mounted\")\n",
    "print(\"Your files are at: /content/drive/MyDrive/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d6bf4c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 4: Configure Dataset Paths\n",
    "\n",
    "**Dataset format required:**\n",
    "\n",
    "```\n",
    "dataset/\n",
    "├── images/\n",
    "│   ├── image_0001.jpg\n",
    "│   ├── image_0002.jpg\n",
    "│   └── ...\n",
    "└── captions.txt\n",
    "```\n",
    "\n",
    "**captions.txt format:**\n",
    "```\n",
    "image_0001.jpg\ta dog running in the park\n",
    "image_0002.jpg\ta cat sleeping on a bed\n",
    "...\n",
    "```\n",
    "\n",
    "**Edit the paths below to match your dataset location.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13bb1a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# ========== CONFIGURE YOUR DATASET PATHS HERE ==========\n",
    "\n",
    "# Option 1: Google Drive path\n",
    "# IMAGE_ROOT = \"/content/drive/MyDrive/your_dataset/images\"\n",
    "# CAPTION_FILE = \"/content/drive/MyDrive/your_dataset/captions.txt\"\n",
    "\n",
    "# Option 2: Uploaded ZIP file path (after extraction)\n",
    "# IMAGE_ROOT = \"/content/dataset/images\"\n",
    "# CAPTION_FILE = \"/content/dataset/captions.txt\"\n",
    "\n",
    "# Option 3: Use sample dataset for testing (5 random images)\n",
    "IMAGE_ROOT = \"/content/blip-using-pvt-cbam/data/sample_images\"\n",
    "CAPTION_FILE = \"/content/blip-using-pvt-cbam/data/captions.txt\"\n",
    "\n",
    "OUTPUT_DIR = \"/content/checkpoints\"\n",
    "\n",
    "print(f\"Images folder: {IMAGE_ROOT}\")\n",
    "print(f\"Captions file: {CAPTION_FILE}\")\n",
    "print(f\"Output dir: {OUTPUT_DIR}\")\n",
    "\n",
    "# Verify paths exist\n",
    "if os.path.exists(IMAGE_ROOT):\n",
    "    num_images = len(os.listdir(IMAGE_ROOT))\n",
    "    print(f\"✓ Images folder found ({num_images} images)\")\n",
    "else:\n",
    "    print(f\"✗ Images folder NOT found at {IMAGE_ROOT}\")\n",
    "    print(\"  Please update IMAGE_ROOT above\")\n",
    "\n",
    "if os.path.exists(CAPTION_FILE):\n",
    "    with open(CAPTION_FILE) as f:\n",
    "        num_captions = len(f.readlines())\n",
    "    print(f\"✓ Captions file found ({num_captions} captions)\")\n",
    "else:\n",
    "    print(f\"✗ Captions file NOT found at {CAPTION_FILE}\")\n",
    "    print(\"  Please update CAPTION_FILE above\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf7bee1",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 5: (Optional) Create Sample Dataset\n",
    "\n",
    "If you don't have a dataset yet, run this to create random sample images for testing.\n",
    "\n",
    "**Skip this if** you already have your dataset configured above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73df03e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "# Create sample dataset with 10 random images\n",
    "sample_dir = \"/content/blip-using-pvt-cbam/data/sample_images\"\n",
    "os.makedirs(sample_dir, exist_ok=True)\n",
    "\n",
    "sample_captions = [\n",
    "    \"a dog running in the park\",\n",
    "    \"a cat sleeping on a bed\",\n",
    "    \"people sitting on a bench\",\n",
    "    \"a sunset over mountains\",\n",
    "    \"a forest path in nature\",\n",
    "    \"a city street at night\",\n",
    "    \"a beach with waves\",\n",
    "    \"a bird flying in sky\",\n",
    "    \"flowers in a garden\",\n",
    "    \"a car parked on street\",\n",
    "]\n",
    "\n",
    "# Create random RGB images\n",
    "for i, caption in enumerate(sample_captions):\n",
    "    img_array = np.random.randint(0, 256, (384, 384, 3), dtype=np.uint8)\n",
    "    img = Image.fromarray(img_array, mode='RGB')\n",
    "    img.save(f\"{sample_dir}/sample_{i:04d}.jpg\")\n",
    "\n",
    "# Create captions file\n",
    "caption_file = \"/content/blip-using-pvt-cbam/data/captions.txt\"\n",
    "with open(caption_file, 'w') as f:\n",
    "    for i, caption in enumerate(sample_captions):\n",
    "        f.write(f\"sample_{i:04d}.jpg\\t{caption}\\n\")\n",
    "\n",
    "print(f\"✓ Created {len(sample_captions)} sample images\")\n",
    "print(f\"✓ Created captions file\")\n",
    "print(f\"\\nSample captions:\")\n",
    "for caption in sample_captions[:3]:\n",
    "    print(f\"  - {caption}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e9d3bcf",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 6: Configure Training Parameters\n",
    "\n",
    "Adjust these based on your hardware and dataset size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c90ea63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== TRAINING CONFIGURATION ==========\n",
    "\n",
    "# For Colab Free Tier (T4 GPU):\n",
    "BATCH_SIZE = 4          # Use 4 for free Colab\n",
    "EPOCHS = 2              # Start with 2 for testing, 8-10 for production\n",
    "LEARNING_RATE = 1e-5    # Standard learning rate\n",
    "\n",
    "# For Colab Pro (V100/A100):\n",
    "# BATCH_SIZE = 8\n",
    "# EPOCHS = 10\n",
    "\n",
    "# Other parameters\n",
    "IMAGE_SIZE = 384        # BLIP standard size\n",
    "USE_AMP = True          # Automatic mixed precision (saves memory)\n",
    "NUM_WORKERS = 0         # Data loader workers (0 for Colab)\n",
    "GRAD_CLIP = 1.0         # Gradient clipping\n",
    "WEIGHT_DECAY = 0.05     # L2 regularization\n",
    "\n",
    "print(\"Training Configuration:\")\n",
    "print(f\"  Batch size: {BATCH_SIZE}\")\n",
    "print(f\"  Epochs: {EPOCHS}\")\n",
    "print(f\"  Learning rate: {LEARNING_RATE}\")\n",
    "print(f\"  Image size: {IMAGE_SIZE}x{IMAGE_SIZE}\")\n",
    "print(f\"  Mixed precision: {USE_AMP}\")\n",
    "print(f\"  Num workers: {NUM_WORKERS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e4e3bdf",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 7: Start Training\n",
    "\n",
    "This runs the main training script. Progress will be printed to the console."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe9d9f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import os\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Build command\n",
    "cmd = [\n",
    "    \"python\", \"train_caption_pvt.py\",\n",
    "    \"--image_root\", IMAGE_ROOT,\n",
    "    \"--caption_file\", CAPTION_FILE,\n",
    "    \"--batch_size\", str(BATCH_SIZE),\n",
    "    \"--epochs\", str(EPOCHS),\n",
    "    \"--lr\", str(LEARNING_RATE),\n",
    "    \"--image_size\", str(IMAGE_SIZE),\n",
    "    \"--output_dir\", OUTPUT_DIR,\n",
    "    \"--num_workers\", str(NUM_WORKERS),\n",
    "    \"--grad_clip\", str(GRAD_CLIP),\n",
    "    \"--weight_decay\", str(WEIGHT_DECAY),\n",
    "]\n",
    ",\n",
    "\n",
    "print(\"Starting training...\")\n",
    "print(f\"Command: {' '.join(cmd)}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "result = subprocess.run(cmd, cwd=\"/content/blip-using-pvt-cbam\")\n",
    "\n",
    "print(\"=\"*80)\n",
    "if result.returncode == 0:\n",
    "    print(\"✓ Training completed successfully!\")\n",
    "else:\n",
    "    print(\"✗ Training failed. Check error messages above.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad699e2e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 8: Find the Trained Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ea41ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "\n",
    "# List all checkpoints\n",
    "checkpoints = sorted(glob.glob(f\"{OUTPUT_DIR}/*.pth\"))\n",
    "\n",
    "if checkpoints:\n",
    "    print(f\"Found {len(checkpoints)} checkpoint(s):\")\n",
    "    for ckpt in checkpoints:\n",
    "        size_mb = os.path.getsize(ckpt) / (1024*1024)\n",
    "        print(f\"  - {os.path.basename(ckpt)} ({size_mb:.1f} MB)\")\n",
    "    \n",
    "    # Use the latest (final_model.pth if it exists, else the last checkpoint)\n",
    "    CHECKPOINT = None\n",
    "    for ckpt in checkpoints:\n",
    "        if 'final_model.pth' in ckpt:\n",
    "            CHECKPOINT = ckpt\n",
    "            break\n",
    "    if CHECKPOINT is None:\n",
    "        CHECKPOINT = checkpoints[-1]\n",
    "    \n",
    "    print(f\"\\n✓ Using checkpoint: {os.path.basename(CHECKPOINT)}\")\n",
    "else:\n",
    "    print(\"✗ No checkpoints found. Run training first.\")\n",
    "    CHECKPOINT = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b09a850",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 9: Run Inference on a Single Image\n",
    "\n",
    "Generate a caption for a test image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8460b241",
   "metadata": {},
   "outputs": [],
   "source": [
    "if CHECKPOINT is None:\n",
    "    print(\"✗ No checkpoint available. Run training first.\")\n",
    "else:\n",
    "    import subprocess\n",
    "    \n",
    "    # Test image URL (a dog)\n",
    "    TEST_IMAGE_URL = \"https://raw.githubusercontent.com/pytorch/hub/master/images/dog.jpg\"\n",
    "    \n",
    "    print(f\"Testing inference on image: {TEST_IMAGE_URL}\\n\")\n",
    "    \n",
    "    cmd = [\n",
    "        \"python\", \"scripts/patch_blip_with_pvt.py\",\n",
    "        \"--checkpoint\", CHECKPOINT,\n",
    "        \"--image_url\", TEST_IMAGE_URL,\n",
    "        \"--device\", device,\n",
    "        \"--max_length\", \"50\",\n",
    "        \"--num_beams\", \"5\",\n",
    "    ]\n",
    "    \n",
    "    result = subprocess.run(cmd, cwd=\"/content/blip-using-pvt-cbam\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86aa656f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 10: Batch Inference on Multiple Images\n",
    "\n",
    "Process multiple images and save results to a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7490b6b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if CHECKPOINT is None:\n",
    "    print(\"✗ No checkpoint available. Run training first.\")\n",
    "else:\n",
    "    import subprocess\n",
    "    \n",
    "    BATCH_OUTPUT = \"/content/batch_results.txt\"\n",
    "    \n",
    "    # Get a few sample images from the training dataset\n",
    "    import glob as glob_module\n",
    "    test_images = sorted(glob_module.glob(f\"{IMAGE_ROOT}/*.jpg\"))[:3]\n",
    "    test_images += sorted(glob_module.glob(f\"{IMAGE_ROOT}/*.png\"))[:max(0, 3 - len(test_images))]\n",
    "    \n",
    "    print(f\"Running batch inference on {len(test_images)} images...\\n\")\n",
    "    \n",
    "    results = []\n",
    "    for i, img_path in enumerate(test_images, 1):\n",
    "        print(f\"[{i}/{len(test_images)}] Processing: {os.path.basename(img_path)}\")\n",
    "        \n",
    "        cmd = [\n",
    "            \"python\", \"scripts/patch_blip_with_pvt.py\",\n",
    "            \"--checkpoint\", CHECKPOINT,\n",
    "            \"--image_path\", img_path,\n",
    "            \"--device\", device,\n",
    "            \"--max_length\", \"50\",\n",
    "            \"--num_beams\", \"5\",\n",
    "        ]\n",
    "        \n",
    "        result = subprocess.run(\n",
    "            cmd,\n",
    "            cwd=\"/content/blip-using-pvt-cbam\",\n",
    "            capture_output=True,\n",
    "            text=True\n",
    "        )\n",
    "        \n",
    "        # Parse caption from output\n",
    "        output = result.stdout + result.stderr\n",
    "        if \"Caption\" in output:\n",
    "            lines = output.split('\\n')\n",
    "            for j, line in enumerate(lines):\n",
    "                if \"Caption\" in line:\n",
    "                    caption = lines[j+1].strip() if j+1 < len(lines) else \"[Failed]\"\n",
    "                    break\n",
    "        else:\n",
    "            caption = \"[Failed to generate]\"\n",
    "        \n",
    "        results.append((os.path.basename(img_path), caption))\n",
    "        print(f\"  Caption: {caption[:100]}...\\n\")\n",
    "    \n",
    "    # Save results\n",
    "    with open(BATCH_OUTPUT, 'w') as f:\n",
    "        for img_name, caption in results:\n",
    "            f.write(f\"Image: {img_name}\\n\")\n",
    "            f.write(f\"Caption: {caption}\\n\\n\")\n",
    "    \n",
    "    print(f\"✓ Results saved to: {BATCH_OUTPUT}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "038b8f33",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 11: View Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d07e43c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "BATCH_OUTPUT = \"/content/batch_results.txt\"\n",
    "\n",
    "if os.path.exists(BATCH_OUTPUT):\n",
    "    print(\"BATCH INFERENCE RESULTS\")\n",
    "    print(\"=\"*80)\n",
    "    with open(BATCH_OUTPUT) as f:\n",
    "        print(f.read())\n",
    "else:\n",
    "    print(\"Results file not found yet. Run batch inference first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b097e8d4",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 12: Download Results & Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a98fcf2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "import os\n",
    "\n",
    "print(\"Preparing files for download...\\n\")\n",
    "\n",
    "# Download batch results\n",
    "if os.path.exists(BATCH_OUTPUT):\n",
    "    print(f\"Downloading: {os.path.basename(BATCH_OUTPUT)}\")\n",
    "    files.download(BATCH_OUTPUT)\n",
    "\n",
    "# Download trained checkpoint\n",
    "if CHECKPOINT and os.path.exists(CHECKPOINT):\n",
    "    print(f\"Downloading: {os.path.basename(CHECKPOINT)}\")\n",
    "    files.download(CHECKPOINT)\n",
    "\n",
    "print(\"\\n✓ Download complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebc4e67d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "### What you've accomplished:\n",
    "1. ✓ Set up BLIP+PVT environment\n",
    "2. ✓ Trained the image captioning model\n",
    "3. ✓ Generated captions for test images\n",
    "4. ✓ Saved results and checkpoint\n",
    "\n",
    "### Results Quality Tips:\n",
    "- **2 epochs on 10 images:** Basic captions, some noise\n",
    "- **5-8 epochs on 10K images:** Good quality, sensible descriptions  \n",
    "- **8-10 epochs on 30K images:** Excellent quality, fluent captions\n",
    "- **15+ epochs on 100K images:** Production-ready\n",
    "\n",
    "### For Better Results:\n",
    "1. Use a larger dataset (30K+ images)\n",
    "2. Train for more epochs (8-15)\n",
    "3. Use professional captions (Flickr30K, COCO dataset)\n",
    "4. Adjust generation parameters if needed:\n",
    "   - `--num_beams 5` (higher = better quality, slower)\n",
    "   - `--repetition_penalty 2.0` (prevents token repetition)\n",
    "   - `--max_length 50` (maximum caption length)\n",
    "\n",
    "### Troubleshooting:\n",
    "- **Out of Memory:** Reduce batch_size to 2\n",
    "- **Slow Training:** Use num_workers=0 (already set)\n",
    "- **Bad Captions:** Train for more epochs with more data\n",
    "- **Repetitive Output:** Increase repetition_penalty in inference"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
